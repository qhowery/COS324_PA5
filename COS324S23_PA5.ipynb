{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qhowery/COS324_PA5/blob/main/COS324S23_PA5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F62B_4pcRI-"
      },
      "source": [
        "# Programming Assignment 5: Value Iteration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "052JVvHvcbQ9"
      },
      "source": [
        "## Problem Setting\n",
        "\n",
        "**Gridworld**: In this assignment, we use the popular RL toolkit  [OpenAI Gym](https://gym.openai.com/)  to implement _value iteration_ on a gridworld environment (Lecture 21). In the gridworld environment we consider, an agent needs to navigate on a 2-D plane by using 4 actions (left, right, up, and down). It starts at a designated state (starting point \"S\"), and needs to try and reach a goal state (\"G\"). The agent controls the movement of a character in a grid world. Some tiles on the grid are walkable (\"S\", \"G\", and \"F\"), and others (\"H\") lead to the agent falling into the water, which ends the episode. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
        "\n",
        "**Rewards**: The environment is such that reaching the goal state gives the agent a `+1` reward, and it gets a `0` reward in other states (detailed description below). Hence, reaching the goal state is equivalent to maximizing the rewards it can accumulate.\n",
        "\n",
        "**Uncertainty**: The environment also has some uncertainty. That is, the movement direction of the agent is uncertain and only partially depends on the chosen direction. In our setting (detailed below), each tile in the grid is \"slippery\", which corresponds to the following uncertainty; if the agent chose to move up/down, it will be able to do so with probability `1/3`, but with probability `2/3` it will move right/left (`1/3` right, `1/3` left). Similarly for the case that the agent chose to move right/left, it might move up/down instead. However, you will always move by 1 tile only (i.e., no \"jumps\"). \n",
        "\n",
        "This is visualized below:\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1Z6DAAiQSPxBbH7jbh-znuk-zqCNQZN0W)\n",
        "\n",
        "In a) the agent (penguin) tries to walk down, in b) it tries to walk right, and c) shows a potential pitfall where just by walking by a hole \"H\" (and not directly into it), the agent still has a `1/3` chance to fall into the hole, which ends the episode with no reward.\n",
        "\n",
        "\n",
        "Note that: \n",
        "* The specific transition probabilities don't matter much here, but it is important to keep in mind is that the action you choose to move towards only partially determined where you actually move, due to the uncertianty (\"slipperiness\") of the environment. \n",
        "\n",
        "* Recall that while you can see the whole gridworld (all the states), the agent only has information about its current state!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t19XeaxoQJiI"
      },
      "source": [
        "## Environment: Frozen Lake \n",
        "Winter is here. You and your friends are tossing around a frisbee at the park when you make a wild throw that leaves the frisbee stranded out in the middle of\n",
        "a lake. The water is mostly frozen (\"F\"), but there are a few holes (\"H\") where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend, and sometimes slip into another, adjacent tile! The surface is described using a grid like the following $8 \\times 8$ example:\n",
        "\n",
        "    SFFFFFFF\n",
        "    FFFFFFFF\n",
        "    FFFHFFFF\n",
        "    FFFFFHFF\n",
        "    FFFHFFFF\n",
        "    FHHFFFHF\n",
        "    FHFFHFHF\n",
        "    FFFHFFFG\n",
        "\n",
        "\n",
        "    S : starting point, safe\n",
        "    F : frozen surface, safe\n",
        "    H : hole, fall to your doom\n",
        "    G : goal, where the frisbee is located\n",
        "\n",
        "\n",
        "The episode ends when you reach the goal or fall in a hole \"H\". You receive a reward of 1 if you reach the goal, and 0 otherwise. The steps that you can make are one of the following: LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3.\n",
        "\n",
        "## Visualization\n",
        "It is easier to understand how this environment behaves if you see it first.  \n",
        "\n",
        "**HINT:** We **strongly recommend** that before implementing anything, you start by first running all cells of this notebook, and scroll down to the cell under **ACT7** see how a **totally-random** agent moves around the frozen lake.\n",
        " \n",
        "You will see it fail many times. In what follows, we will implement a much better agent that can navigate to the goal. **You can use the visualization to see how your agent performs, where it fails, and debug issues that arise during development.**\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCQ3b6BQQJiI"
      },
      "source": [
        "## Notation\n",
        "\n",
        "Mapping of the OpenAI gym constructs to the notation used in lectures:\n",
        "* The value function $V(s)$, is implemented here as a small array, of size corrsponsing to number of states, such that `V[s]` contains the value $V(s)$. \n",
        "* The transition probabilities of the environment $P(s'|s,a)$ correspond to the gym object `env.P` described below. \n",
        "* In lectures, the reward function $r(a|s, s')$ signified  taking action $a$ while in state $s$ moving to state $s'$. However, in our setting here the reward fixed into the environment is such that $r(a|s=\\text{'G'}, s')=1$ and $r(a|s \\neq\\text{'G'}, s')=0$ for any $a, s'$. In other words, in this setting we only get the reward if we *actually reach* the goal state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2"
      },
      "source": [
        "### First, install dependencies (takes ~1 min)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8-AxnvAVyzQQ"
      },
      "outputs": [],
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay pygame > /dev/null 2>&1\n",
        "!apt-get install -y xvfb libav-tools python-opengl ffmpeg > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pdb2JwZy4jGj"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "import pygame\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogyVgSS0Qlx_",
        "outputId": "d60eeb2d-8234-456e-9661-9f305669a6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Surface(640x480x32 SW)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# setting up dummy display driver for Colab\n",
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "pygame.display.set_mode((640,480))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dGEFMfDOzLen"
      },
      "outputs": [],
      "source": [
        "# first, we set some useful parameters: \n",
        "gamma = 0.99        # discount factor\n",
        "theta = 0.000001    # precision of value_iteration\n",
        "        \n",
        "# let's set up the frozen lake environment. \n",
        "env = gym.make(\"FrozenLake8x8-v1\", new_step_api=True) # create the environment\n",
        "env = env.unwrapped # enwrap it to have additional information from it\n",
        "\n",
        "# spaces dimension\n",
        "num_actions = env.action_space.n # we can move up/down/left/right, i.e., 4 actions.\n",
        "num_states = env.observation_space.n # we have one state per tile in the grid (64 states) \n",
        "\n",
        "# NOTE, the transition probabilities we mentioned above (i.e., the \"slipperiness\"), are already given \n",
        "# in the env.P object. Specifically, env.P are the transition probabilities (dictionary dict of dicts of lists)\n",
        "#           P[s][a] == [(probability, s', reward, done), \n",
        "#                       (probability, s', reward, done), \n",
        "#                       (probability, s', reward, done), ]\n",
        "\n",
        "# As an example, for state s=0 (the starting tile \"S\"),and action a=0 (LEFT), you will have:\n",
        "#           P[0][0] == [(1/3, 0, 0, False), (1/3, 0, 0, False), (1/3, 4, 0, False)] \n",
        "# since:\n",
        "# with probability 1/3 you'll move UP (but there's no up, so you'll stay put at s=0),\n",
        "# with probability 1/3 you'll move LEFT (but there's no left, so you'll stay put at s=0),\n",
        "# with probability 1/3 you'll move DOWN (so you'll get to the tile below, which is s'=4).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDP6Ff27k_uk"
      },
      "source": [
        "## Value-Iteration algorithm\n",
        "\n",
        "We will now implement the Value Iteration algorithm you have seen in class. The pseudo-code is given below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPLXehxpQJiL"
      },
      "source": [
        "### Pseudo-code\n",
        "1. Parameter: a small threshold $\\theta>0$ determining accuracy of estimation.\n",
        "1. Initialize $V(s)=-\\frac{M}{1-\\gamma}$, for all $s\\in\\mathcal{S}$, where $M$ is the upper bound on the absolute value of immediate rewards.\n",
        "1. **Loop**:\n",
        "    1. $\\Delta \\leftarrow 0$.\n",
        "    1. **Loop for each** $s\\in\\mathcal{S}$:\n",
        "        1. $v\\leftarrow V(s)$.\n",
        "        1. $V(s) \\leftarrow \\max_a\\sum_{s'} p(s'\\mid s, a)\\bigl[r(a\\mid s, s') + \\gamma V(s')\\bigr]$.\n",
        "        1. $\\Delta\\leftarrow \\max(\\Delta, \\lvert v - V(s) \\rvert)$.\n",
        "1. **until** $\\Delta < \\theta$.\n",
        "1. Output a deterministic policy, $\\pi\\approx\\pi_*$, such that\n",
        "$$ \\pi(s) = \\text{argmax}_a \\sum_{s'} p(s'\\mid s, a)\\bigl[r(a\\mid s, s') + \\gamma V(s')\\bigr].$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZrrQafMQJiL"
      },
      "source": [
        "## ACTS 1-2: Computing the sum & argmax of Value-Iteration. \n",
        "\n",
        "Compute the inner sum $$\\sum_{s'} p(s'\\mid s, a)\\bigl[r(a\\mid s, s') + \\gamma V(s')\\bigr],$$ given $V, s, a, \\gamma$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5topu9WrLNo_"
      },
      "outputs": [],
      "source": [
        "# ACT 1: compute the inner sum of the equations above, given:\n",
        "        # Value function V (an array of size num_states),\n",
        "        # State s, Action a, and discount factor gamma.\n",
        "        \n",
        "def sum_over_states(V, s, a, gamma):\n",
        "    sum_val = 0\n",
        "    # recall that env.P is a list of tuples for (s,a): [(prob, s', r, done),...] (See block above for more details!)\n",
        "    # where 'prob' is the transition probability P(s'|s,a). \n",
        "    # also, since r is associated with the states, you can think of the sum as only traversing states s'. \n",
        "    # therefore, all is needed is to iterate over each possible transition to state s'\n",
        "    # from the pair (s,a), and compute the sum value (as in the pseudo-code).\n",
        "    ### YOUR CODE GOES HERE\n",
        "    for state in env.P[s][a]:\n",
        "      p = state[0]\n",
        "      s = state[1]\n",
        "      r = state[2]\n",
        "      sum_val += p * (r + (gamma * V[s]))\n",
        "    return sum_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzRc5WrWQJiM"
      },
      "source": [
        "We now compute the argmax operation, in which we will update $\\pi(a|s)$ where $s$ is the given state, and $a$ is the maximizing action! To obtain it, you should use the function above `sum_over_states`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b7WLWpUMQJiM"
      },
      "outputs": [],
      "source": [
        "# ACT 2: update the action which can take us to a higher valued state, given:\n",
        "        # Value function V (an array of size num_states),\n",
        "        # Policy array pi (a matrix of shape [num_states, num_actions]),\n",
        "        # State s, and discount factor gamma.\n",
        "        \n",
        "def argmax_over_actions(V, pi, s, gamma):\n",
        "    max_val = 0\n",
        "    ### YOUR CODE GOES HERE\n",
        "    max_action = 0\n",
        "    for a in range(len(pi[s])):\n",
        "      val = sum_over_states(V, s, a, gamma)\n",
        "      if (val > max_val):\n",
        "        max_val = val\n",
        "        max_action = a\n",
        "    pi[s][:] = 0\n",
        "    pi[s][max_action] = 1\n",
        "    return pi, max_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LXHKhMPQJiM"
      },
      "source": [
        "## ACT 3: Computing the Bellman update\n",
        "\n",
        "We now perform a single iteration of update to the value function $V$, given state $s$, and discount factor $\\gamma$. \n",
        "You may call a function you have implemented above, and output the difference between the new, updated value for this state, and the previous value (denoted as `delta` in the pseudo-code above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TADCWSLfXB4",
        "outputId": "15f8eaa0-5766-4c93-e9ae-0aa86e6786cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# ACT 3: update the Value function for state s, that is: V[s], by taking action which maximizes current value. Given:\n",
        "        # Value function V (an array of size num_states),\n",
        "        # State s, and discount factor gamma.\n",
        "\n",
        "def bellman_optimality_update(V, s, gamma):  \n",
        "    pi = np.zeros((num_states, num_actions))     \n",
        "    ### YOUR CODE GOES HERE\n",
        "    # ACT3a: call a function implemented above, to get the action which maximizes current value.\n",
        "    pi, max_val = argmax_over_actions(V, pi, s, gamma)\n",
        "    # ACT3b: update value V[s]\n",
        "    old_val = V[s]\n",
        "    V[s] = max_val\n",
        "    # ACT 3c: compute and output delta.\n",
        "    delta = abs(old_val - V[s])\n",
        "    return delta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2xStxIWQJiM"
      },
      "source": [
        "## ACT 4: Initialize V\n",
        "Initialize $V$ such that for all states, $V[s] = -\\frac{M}{1-\\gamma}$, where $M$ is the upper bound on the absolute value of immediate rewards to be filled in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMmwrAeWQJiN",
        "outputId": "7634e117-4328-4c7c-c0b4-29356422e0c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# ACT 4: Initialize V\n",
        "# length_V gives the size of the vector V\n",
        "def init_V(gamma, M, length_V):\n",
        "    ### YOUR CODE GOES HERE\n",
        "    V = np.zeros(length_V)\n",
        "    V.fill(-1 * M / (1 - gamma))\n",
        "    return V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gXAhs3cQJiN"
      },
      "source": [
        "## ACT 5-6: Value-iteration (putting it all together)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsGJ0nSieiix",
        "outputId": "decee844-3e23-4865-a787-e6f69adc8470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Now, let's put it all together. Recall that we are given:\n",
        "        # discount factor gamma, and wanted precision theta.\n",
        "def value_iteration(gamma, theta):\n",
        "    # Initialize V with init_V function\n",
        "    ### YOUR CODE GOES HERE\n",
        "    V = init_V(gamma, 1, num_states)\n",
        "    # ACT 5: construct the main loop, \n",
        "    #        iteratively calling the bellman update,\n",
        "    #        and break when sufficient accuracy was reached. \n",
        "    while True: \n",
        "        delta = 0\n",
        "        ### YOUR CODE GOES HERE\n",
        "        for s in range(len(V)):\n",
        "          v = V[s]\n",
        "          update = bellman_optimality_update(V, s, gamma)\n",
        "          delta = max(update, delta)\n",
        "        if (delta < theta):\n",
        "          break\n",
        "\n",
        "    pi = np.zeros((num_states, num_actions)) \n",
        "    # ACT 6: Extract optimal policy using the \n",
        "    # argmax_over_actions function defined in ACT 2\n",
        "    ### YOUR CODE GOES HERE\n",
        "    for s in range(len(V)):\n",
        "      pi, max_val = argmax_over_actions(V, pi, s, gamma)\n",
        "    # output optimal value funtion, optimal policy\n",
        "    return V, pi                                  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MvIfGO3QJiN"
      },
      "source": [
        "## Helper functions (**no action needed**)\n",
        "This next two functions `show_state` and `pretty_print` are helper functions we provide that print and visualize state information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WaSJKClYRpAw"
      },
      "outputs": [],
      "source": [
        "def show_state(env, trial=0, step=0):\n",
        "    plt.figure(5)\n",
        "    plt.clf()\n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    plt.title(f\"Trial: {trial} | Step: {step}\")\n",
        "    plt.axis('off')\n",
        "    clear_output(wait=True)\n",
        "    display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Y2GuhYhn4Gtb"
      },
      "outputs": [],
      "source": [
        "def pretty_print(i_episode, t, done, reward, total_rewards, debug=False):\n",
        "  if debug:\n",
        "    show_state(env, i_episode, t)\n",
        "    if done:\n",
        "        print('')\n",
        "        if reward == 1:\n",
        "            total_rewards += 1\n",
        "            print(\">> Success!! got the goal\")\n",
        "        else:\n",
        "            print(\">> Failed!! fell into a hole\")\n",
        "        time.sleep(3), clear_output(wait=True)\n",
        "    else:\n",
        "      time.sleep(.5)\n",
        "    it = i_episode + 1\n",
        "    print(f\"Avg reward so far = {(total_rewards / it):.2f}\")\n",
        "  else:\n",
        "    if done:\n",
        "      if reward == 1:\n",
        "        total_rewards += 1\n",
        "      it = i_episode + 1\n",
        "      if it%10 == 0:\n",
        "        print(f\"Avg reward obtained in iteration {it} = {(total_rewards / it):.2f}\")\n",
        "  return total_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZpAonqnQJiN"
      },
      "source": [
        "## ACT 7: Call agent, and evaluate results\n",
        "\n",
        "Below is the main loop of our setting: first, we call the `value_iteration` function, and obtain the optimal policy. \n",
        "Then, we run in many episodes (or trials) of the algorithm. In each trial, we start at the starting state \"S\". We then ask the agent what action to go towards. We make a step in the frozen lake with the chosen action (but may not move there because the lake is slippery!), our next state is given after we call `env.step(action)`. \n",
        "\n",
        "You can expect to get an average reward in the range (0.8,1) upon correct implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8nj5sjsk15IT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c1e35d-284f-41ce-bf1a-351989053db7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg reward obtained in iteration 10 = 1.00\n",
            "Avg reward obtained in iteration 20 = 0.95\n",
            "Avg reward obtained in iteration 30 = 0.93\n",
            "Avg reward obtained in iteration 40 = 0.95\n",
            "Avg reward obtained in iteration 50 = 0.94\n",
            "Avg reward obtained in iteration 60 = 0.92\n",
            "Avg reward obtained in iteration 70 = 0.93\n",
            "Avg reward obtained in iteration 80 = 0.94\n",
            "Avg reward obtained in iteration 90 = 0.93\n",
            "Avg reward obtained in iteration 100 = 0.93\n"
          ]
        }
      ],
      "source": [
        "# note that you can first try it out with a random agent, by setting this variable to True. \n",
        "random_agent = False\n",
        "episodes = 100\n",
        "\n",
        "# When `debug` is set to True, you can see visualizations of the agent's actions\n",
        "# While running over 100 episodes, set this to False to speed up your execution\n",
        "debug = False\n",
        "\n",
        "if not random_agent:\n",
        "    # We have called the value_iteration function:\n",
        "    V_, pi = value_iteration(gamma, theta)\n",
        "    # note that we don't need the value function V anymore, as we already extracted the optimal policy!\n",
        "\n",
        "total_rewards = 0\n",
        "for i_episode in range(episodes):\n",
        "      state = env.reset()\n",
        "      t = 0\n",
        "      while True:\n",
        "          t+=1\n",
        "          # your agent picks an action: \n",
        "          if random_agent:\n",
        "            action = env.action_space.sample() # totally random action !\n",
        "          else:\n",
        "            # ACT 7: get the best action according to optimal policy `pi`, and current state `state`.\n",
        "            action = 0\n",
        "            ### YOUR CODE GOES HERE\n",
        "            action = np.nonzero(pi[state])\n",
        "            action = action[0][0]\n",
        "              \n",
        "          # make a step according to policy\n",
        "          state, reward, terminated, truncated, info = env.step(action)\n",
        "          # the above variables are:\n",
        "                # state (object): agent's observation, current state (new state we've reached after the step we took)\n",
        "                # reward (float) : amount of reward returned after previous action\n",
        "                # done (bool): whether the episode has ended\n",
        "                # terminated (bool): whether a `terminal state` is reached (only True if the state is the Goal or a Hole!)\n",
        "                # truncated (bool): whether a truncation condition outside the scope of the MDP is satisfied (eg. timelimit/agent physically going out of bounds)\n",
        "                # info (dict): contains auxiliary diagnostic information (might be helpful for debugging, though not used in this assignment)\n",
        "\n",
        "          done = truncated or terminated # we are done in either case\n",
        "\n",
        "          # visualization\n",
        "          total_rewards = pretty_print(i_episode, t, done, reward, total_rewards, debug=debug)\n",
        "          if done: break\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZtURZfCQJiN"
      },
      "source": [
        "## Conceptual Questions (ACTs 8-10)\n",
        "\n",
        "Notice that in your implementation above, an agent that follows the value-iteration policy performs much better than an agent that follows the random policy (you can run both variations by setting the `random_agent` variable to `True/False`). \n",
        "\n",
        "Please **briefly** answer the following questions (1-2 sentences per question suffices)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0beR-B-QJiO"
      },
      "source": [
        "#### **ACT8:** Does the value-iteration policy *always* obtains reward=1? Explain why/why not.\n",
        "#### <span style='color:cyan'>  **Answer:** </span> No, since the probability of reaching the ideal state is not always 1 so therefore the reward has a chance of not being 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68gglBTJQJiO"
      },
      "source": [
        "#### **ACT9:** Will the random policy *always* obtain reward=0? Explain why/why not.\n",
        "#### <span style='color:cyan'>  **Answer:** </span> No because there is the possibility of the policy selecting the optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdSUi0u3QJiO"
      },
      "source": [
        "#### **ACT10:** When averaged over many episodes, will the value-iteration policy always obtain a larger reward than the random policy? Explain why/why not (no need to prove it, just state the appropriate claim shown in lecture and breifly explain how it implies this statement). \n",
        "#### <span style='color:cyan'>  **Answer:** </span> From lecture: the reward of following the optimal policy is greater than or equal to the reward of following a random policy. So therefore averaging the value-iteration policy over many episodes should result in a larger reward than the random policy."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}